Wide & Deep是推荐系统的经典模型

原始论文中已经说得很清楚了，笔者在这里说一说自己对Wide & Deep的理解：

从模型内容Pooling的思路出发，Wide & Deep是可以用残差网络解释的，并且从一定程度上说，Wide & Deep 自带残差网络。为什么？

F(x) = f(x) + x， f(x)可视为DNN（非线性），x可视为LR（线性）

从数学公式看，如果没有非线性激活函数，残差网络存在与否意义不大。如果残差网络存在，则只是做了简单的平移：

增加非线性激活函数之后，y=W_2 σ(W_1 x)+x=(〖σW〗_1 W_2+1)x，模型的特征表达能力大幅提升。这也是为什么Residual Block有2个权重（W_1,W_2）的原因。

工程上来说，该模块在原始论文中提到由特别强的记忆能力，笔者认为，这应该是是依靠LR强大的“评分卡”实现的，此外，该模块也赋予模型强大的可解释性能力

在不追求复杂模型那么一点点准确率的前提下，DNN部分作为LR的补充，兼具可解释性和高阶特征提取能力。
